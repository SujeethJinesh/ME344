{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee4f78c-2fcc-414e-be8c-d3a2b7d5bf5d",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 3.1 8B\n",
    "\n",
    "Adapted from source: https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z#scrollTo=aTaDCGTe78bK\n",
    "and\n",
    "https://huggingface.co/docs/trl/sft_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3a9b4-79e8-4791-97c7-42f5c8305873",
   "metadata": {},
   "source": [
    "## 0. Check Internet Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244ecac8-7912-444b-adff-6f0a11d326f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet is on\n"
     ]
    }
   ],
   "source": [
    "# check internet connection\n",
    "import requests\n",
    " \n",
    "# initializing URL\n",
    "url = \"https://www.google.com\"\n",
    "timeout = 10\n",
    "try:\n",
    "    # requesting URL\n",
    "    request = requests.get(url,\n",
    "                           timeout=timeout)\n",
    "    print(\"Internet is on\")\n",
    " \n",
    "# catching exception\n",
    "except (requests.ConnectionError,\n",
    "        requests.Timeout) as exception:\n",
    "    print(\"Internet is off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ae4c3-c56a-487c-b485-2a5f0d138f77",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1bd1d3-c0ef-4cd2-ab45-33ae23d7a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "!export HF_TOKEN=***REMOVED***\n",
    "!echo $HF_TOKEN\n",
    "\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20121965-95d5-4e78-bde5-62a70740937c",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5964f-aff5-4119-a799-96e80a70bdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 6.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d1889d3b5349f889afffbf6f2d8041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "max_seq_length = 512\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# Prepare model for PEFT\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\"\n",
    ")\n",
    "\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6208a13-bd06-4367-8000-5eb535ccc44b",
   "metadata": {},
   "source": [
    "## 3. Prepare data and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c907c6-22ac-45ee-86b1-6b6a4d27f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}\n",
    ")\n",
    "\n",
    "def apply_template(examples):\n",
    "    messages = examples[\"conversations\"]\n",
    "    print(f\"message is {message}\")\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
    "    return {\"text\": text}\n",
    "\n",
    "# TODO: Edit this dataset to be \"thesherrycode/gen-z-slangs-translation\"\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
    "dataset = dataset.map(apply_template, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88634298-0aa0-47a2-9228-f41447680e49",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765f409-177a-44e8-a797-70c311a9001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    args=TrainingArguments(\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=10,\n",
    "        output_dir=\"output\",\n",
    "        seed=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c693f195-f3d9-4d0e-ab8d-70b5d5d69269",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eca781-ecd0-41de-b188-b5409686c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd1806-4433-4879-9e93-e077e372e3c3",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945dc18-4ca2-44d9-993b-8a54d230cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"trained_model\", tokenizer, save_method=\"merged_16bit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
