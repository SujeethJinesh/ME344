#!/usr/bin/env python3
"""
End-to-end RAG pipeline test script
Tests the complete RAG workflow: Query -> Vector Search -> Context Retrieval -> LLM Generation
"""

import json
import chromadb
import chromadb.utils.embedding_functions as embedding_functions
import requests
import time

def test_rag_pipeline():
    print("ğŸ§ª Testing Complete RAG Pipeline")
    print("=" * 50)
    
    # Test 1: ChromaDB Connection and Data
    print("\n1ï¸âƒ£ Testing ChromaDB Connection...")
    try:
        client = chromadb.HttpClient(host='localhost', port=8000)
        client.heartbeat()
        print("âœ… ChromaDB connection successful")
        
        collections = client.list_collections()
        print(f"ğŸ“‹ Found {len(collections)} collections")
        
        # Get the RAG collection
        embedder = embedding_functions.OllamaEmbeddingFunction(
            url="http://localhost:11434/api/embeddings",
            model_name="nomic-embed-text",
        )
        
        collection = client.get_collection(
            name='llm_rag_collection',
            embedding_function=embedder,
        )
        
        count = collection.count()
        print(f"âœ… Found 'llm_rag_collection' with {count} documents")
        
    except Exception as e:
        print(f"âŒ ChromaDB test failed: {e}")
        return False
    
    # Test 2: Vector Search
    print("\n2ï¸âƒ£ Testing Vector Search...")
    test_queries = [
        "what is cool",
        "define janky", 
        "meaning of sick",
        "what does fire mean"
    ]
    
    search_results = {}
    
    for query in test_queries:
        try:
            print(f"ğŸ” Searching for: '{query}'")
            results = collection.query(
                query_texts=[query],
                n_results=2,
            )
            
            if results and results['documents'] and results['documents'][0]:
                search_results[query] = results
                print(f"âœ… Found {len(results['documents'][0])} results")
                
                for i, doc in enumerate(results['documents'][0]):
                    distance = results['distances'][0][i] if results['distances'] else 0
                    similarity = 1 - distance
                    print(f"   {i+1}. (similarity: {similarity:.3f}): {doc[:80]}...")
            else:
                print(f"âš ï¸ No results found for '{query}'")
                
        except Exception as e:
            print(f"âŒ Search failed for '{query}': {e}")
            return False
    
    # Test 3: RAG Query Augmentation
    print("\n3ï¸âƒ£ Testing RAG Query Augmentation...")
    
    def create_augmented_query(query, context_docs):
        context = ' '.join(context_docs)
        augmented = f"Use the following information to answer the question:\n\n{context}\n\nQuestion: {query}"
        return augmented
    
    test_query = "what is cool"
    if test_query in search_results:
        context_docs = search_results[test_query]['documents'][0]
        augmented_query = create_augmented_query(test_query, context_docs)
        
        print(f"ğŸ“ Original query: {test_query}")
        print(f"ğŸ“ Augmented query length: {len(augmented_query)} characters")
        print(f"ğŸ“ Context preview: {augmented_query[:200]}...")
        print("âœ… Query augmentation successful")
    else:
        print(f"âŒ No search results for test query: {test_query}")
        return False
    
    # Test 4: Ollama LLM Generation
    print("\n4ï¸âƒ£ Testing Ollama LLM Generation...")
    
    try:
        # Test basic Ollama connectivity
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            print("âœ… Ollama server accessible")
        else:
            print("âŒ Ollama server not responding")
            return False
        
        # Test LLM generation with augmented query
        payload = {
            "model": "llama3.1",
            "prompt": augmented_query,
            "stream": False
        }
        
        print("ğŸ¤– Sending augmented query to Ollama...")
        start_time = time.time()
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            headers={"Content-Type": "application/json"},
            json=payload,
            timeout=30
        )
        
        generation_time = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            llm_response = result.get('response', '')
            
            print(f"âœ… LLM generation successful ({generation_time:.2f}s)")
            print(f"ğŸ“ Response length: {len(llm_response)} characters")
            print(f"ğŸ“ Response preview: {llm_response[:200]}...")
            
            # Check if response contains relevant information
            if any(word in llm_response.lower() for word in ['cool', 'slang', 'meaning', 'definition']):
                print("âœ… Response appears relevant to the query")
            else:
                print("âš ï¸ Response may not be directly relevant")
                
        else:
            print(f"âŒ LLM generation failed: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Ollama test failed: {e}")
        return False
    
    # Test 5: End-to-End RAG Function
    print("\n5ï¸âƒ£ Testing Complete RAG Function...")
    
    def complete_rag_query(user_query):
        """Complete RAG pipeline function"""
        try:
            # Step 1: Vector search
            results = collection.query(
                query_texts=[user_query],
                n_results=2,
            )
            
            if not results or not results['documents'] or not results['documents'][0]:
                return f"No relevant information found for: {user_query}"
            
            # Step 2: Create augmented query
            context_docs = results['documents'][0]
            context = ' '.join(context_docs)
            augmented_query = f"Use the following information to answer the question:\n\n{context}\n\nQuestion: {user_query}"
            
            # Step 3: LLM generation
            payload = {
                "model": "llama3.1",
                "prompt": augmented_query,
                "stream": False
            }
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'No response generated')
            else:
                return f"LLM generation failed: {response.status_code}"
                
        except Exception as e:
            return f"RAG query failed: {e}"
    
    # Test the complete function
    test_queries_final = [
        "what does cool mean in slang?",
        "explain the term janky"
    ]
    
    for query in test_queries_final:
        print(f"\nğŸ¯ Testing complete RAG for: '{query}'")
        result = complete_rag_query(query)
        print(f"ğŸ“ RAG Response: {result[:300]}{'...' if len(result) > 300 else ''}")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ RAG Pipeline Test Complete!")
    print("âœ… All components are working correctly")
    return True

if __name__ == "__main__":
    success = test_rag_pipeline()
    if success:
        print("\nğŸš€ Ready to test the React frontend!")
        print("ğŸ’¡ Start the frontend with: ./run_part1.sh --frontend-only")
        print("ğŸ’¡ Then visit: http://localhost:3000")
    else:
        print("\nâŒ RAG pipeline has issues that need to be fixed")