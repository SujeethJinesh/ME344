{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Information:\n",
      "Python executable: /Users/sujeethjinesh/Desktop/ME344/summer_2025/.venv/bin/python3.11\n",
      "Python version: 3.11.6 (v3.11.6:8b6ee5ba3b, Oct  2 2023, 11:18:21) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "Current working directory: /Users/sujeethjinesh/Desktop/ME344/summer_2025\n",
      "✓ Using project virtual environment\n"
     ]
    }
   ],
   "source": [
    "# First, let's make sure we're using the right Python environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Python Information:\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if we're in the right virtual environment\n",
    "if '.venv' in sys.executable and 'summer_2025' in sys.executable:\n",
    "    print(\"✓ Using project virtual environment\")\n",
    "else:\n",
    "    print(\"⚠️  Not using project virtual environment!\")\n",
    "    print(\"   To fix: Select the correct kernel in Jupyter (Kernel → Change Kernel → ME344 RAG (Python))\")\n",
    "    print(\"   Or restart with: ./run_part1.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import chromadb\n",
    "import pprint\n",
    "import hashlib\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.evaluation import load_evaluator\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "chroma_path=\"chroma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Setup Notebook\n",
    "\n",
    "## ⚠️ IMPORTANT: Before Running This Notebook\n",
    "\n",
    "1. **Select the correct kernel**: Kernel → Change Kernel → **ME344 RAG (Python)**\n",
    "2. **Make sure all services are running**: Run `./run_part1.sh` in the terminal\n",
    "3. **Run cells in order**: Execute each cell sequentially from top to bottom\n",
    "\n",
    "If you see any import errors or \"module not found\" errors, you're likely using the wrong kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /Users/sujeethjinesh/Desktop/ME344/summer_2025/.venv/bin/python3.11\n",
      "✅ ChromaDB version: 0.4.24\n",
      "✅ LangChain community imports working\n",
      "✅ Requests library available\n"
     ]
    }
   ],
   "source": [
    "# Test imports to ensure we have the right environment\n",
    "import sys\n",
    "print(f\"Python: {sys.executable}\")\n",
    "\n",
    "try:\n",
    "    import chromadb\n",
    "    print(f\"✅ ChromaDB version: {chromadb.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ChromaDB import failed: {e}\")\n",
    "    \n",
    "try:\n",
    "    from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "    print(\"✅ LangChain community imports working\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ LangChain community import failed: {e}\")\n",
    "    \n",
    "try:\n",
    "    import requests\n",
    "    print(\"✅ Requests library available\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Requests import failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Cleaned Data\n",
    "\n",
    "We'll load our cleaned data in the data folder. In our case, we'll be loading in slang data from urban dictionary as a CSV. We encourage you to check out the data to get a sense of how it's laid out.\n",
    "\n",
    "If you want to use your own custom, this is where you'd make that change. Just point the file path to your own data, and change out CSVLoader with whatever loader works best for your use case. You can see the different types of loaders at [LangChain](https://python.langchain.com/docs/integrations/document_loaders/#common-file-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 123172 documents\n"
     ]
    }
   ],
   "source": [
    "# Check if data file exists\n",
    "data_file_path = './data/cleaned_slang_data.csv'\n",
    "if not os.path.exists(data_file_path):\n",
    "    raise FileNotFoundError(f\"Data file not found: {data_file_path}\")\n",
    "\n",
    "loader = CSVLoader(file_path=data_file_path)\n",
    "try:\n",
    "    slang_document = loader.load()\n",
    "    if not slang_document:\n",
    "        raise ValueError(\"No documents loaded from CSV file\")\n",
    "    print(f\"Successfully loaded {len(slang_document)} documents\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded in the data, we can take a quick peek at it to see what we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='word: bank\n",
      "definition: another cash money word' metadata={'source': './data/cleaned_slang_data.csv', 'row': 0}\n",
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "There are 123172 documents\n"
     ]
    }
   ],
   "source": [
    "print(slang_document[0])\n",
    "print(type(slang_document))\n",
    "print(type(slang_document[0]))\n",
    "print(\"There are\", len(slang_document), \"documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have over 600k Slang items! So we're working with quite a large amount of data!\n",
    "\n",
    "## Chunking our data\n",
    "\n",
    "Now let's go ahead and chunk our data. Remember that this is cutting up the data into manageable chunks so we can fit it into our vector database (cheatsheet for the LLM)!\n",
    "\n",
    "Since we're processing over 600k elements, this may take a minute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only necessary if we have too much data to add to the context.\n",
    "def split_documents(documents: list[Document]):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=80,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "  )\n",
    "  return text_splitter.split_documents(documents)\n",
    "\n",
    "chunks = split_documents(slang_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the chunks look like. They should be pretty similar but it does definitely help for some words that have very long definitions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='word: bank\n",
      "definition: another cash money word' metadata={'source': './data/cleaned_slang_data.csv', 'row': 0}\n",
      "124060\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Embedding Function\n",
    "\n",
    "Let's start by creating our embedding function. In our case, we want to use a specialized embedding model so it's fast and efficient to get embeddings. Since this embedding model is different from our LLM inference model, we need to pull it using `ollama pull nomic-embed-text`, which you should have already done from the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach using LangChain's Ollama embeddings\n",
    "def get_langchain_embedding_function():\n",
    "    \"\"\"Get embedding function using LangChain's Ollama integration\"\"\"\n",
    "    try:\n",
    "        from langchain_community.embeddings import OllamaEmbeddings\n",
    "        print(\"Using LangChain's OllamaEmbeddings\")\n",
    "        \n",
    "        embeddings = OllamaEmbeddings(\n",
    "            model=\"nomic-embed-text\",\n",
    "            base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "        \n",
    "        # Wrap it to be compatible with ChromaDB's expected interface\n",
    "        class ChromaDBCompatibleEmbeddings:\n",
    "            def __init__(self, langchain_embeddings):\n",
    "                self.embeddings = langchain_embeddings\n",
    "            \n",
    "            def __call__(self, input):\n",
    "                if isinstance(input, str):\n",
    "                    return [self.embeddings.embed_query(input)]\n",
    "                else:\n",
    "                    return self.embeddings.embed_documents(input)\n",
    "        \n",
    "        return ChromaDBCompatibleEmbeddings(embeddings)\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"LangChain Ollama embeddings not available: {e}\")\n",
    "        print(\"Install with: pip install langchain-community\")\n",
    "        raise\n",
    "\n",
    "# Uncomment this line to use LangChain embeddings instead:\n",
    "# get_embedding_function = get_langchain_embedding_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using LangChain Ollama Embeddings\n",
    "\n",
    "If you're having issues with ChromaDB's embedding functions, you can also use LangChain's Ollama embeddings which are more compatible across versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"Get Ollama embedding function with fallback for different ChromaDB versions\"\"\"\n",
    "    try:\n",
    "        # Try ChromaDB 0.5.x+ style first\n",
    "        import chromadb.utils.embedding_functions as ef\n",
    "        if hasattr(ef, 'OllamaEmbeddingFunction'):\n",
    "            print(\"Using ChromaDB's OllamaEmbeddingFunction\")\n",
    "            embeddings = ef.OllamaEmbeddingFunction(\n",
    "                url=\"http://localhost:11434/api/embeddings\",\n",
    "                model_name=\"nomic-embed-text\",\n",
    "            )\n",
    "            return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"ChromaDB OllamaEmbeddingFunction not available: {e}\")\n",
    "    \n",
    "    # Fallback: Use custom implementation\n",
    "    print(\"Using custom Ollama embedding function\")\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    \n",
    "    class CustomOllamaEmbeddings:\n",
    "        def __init__(self, url=\"http://localhost:11434\", model_name=\"nomic-embed-text\"):\n",
    "            self.url = url\n",
    "            self.model_name = model_name\n",
    "        \n",
    "        def __call__(self, input):\n",
    "            \"\"\"\n",
    "            Generate embeddings for input text(s)\n",
    "            Args:\n",
    "                input: single string or list of strings\n",
    "            Returns:\n",
    "                list of embeddings (each embedding is a list of floats)\n",
    "            \"\"\"\n",
    "            if isinstance(input, str):\n",
    "                texts = [input]\n",
    "            else:\n",
    "                texts = input\n",
    "                \n",
    "            embeddings = []\n",
    "            for text in texts:\n",
    "                try:\n",
    "                    response = requests.post(\n",
    "                        f\"{self.url}/api/embeddings\",\n",
    "                        json={\"model\": self.model_name, \"prompt\": text},\n",
    "                        timeout=30\n",
    "                    )\n",
    "                    if response.status_code == 200:\n",
    "                        embedding = response.json()[\"embedding\"]\n",
    "                        embeddings.append(embedding)\n",
    "                    else:\n",
    "                        raise Exception(f\"Ollama API error: {response.status_code} - {response.text}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting embedding for text: {e}\")\n",
    "                    # Return zero vector as fallback\n",
    "                    embeddings.append([0.0] * 768)  # nomic-embed-text produces 768-dim embeddings\n",
    "                    \n",
    "            return embeddings\n",
    "    \n",
    "    return CustomOllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom Ollama embedding function\n",
      "Embedding dimension: 768\n",
      "First 10 values: [1.518409252166748, 1.6257004737854004, -3.643442153930664, -0.38027846813201904, 0.4524954557418823, -0.8147395849227905, 0.3174838125705719, -0.46331557631492615, 0.07126598060131073, -0.31656473875045776]\n"
     ]
    }
   ],
   "source": [
    "# Use the robust embedding function\n",
    "embedding_function = get_embedding_function()\n",
    "\n",
    "chunk = chunks[0].page_content\n",
    "\n",
    "# Test the embedding function\n",
    "embeddings = embedding_function([chunk])\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"First 10 values: {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Embeddings\n",
    "\n",
    "Let's see what an embedding looks like for reference with our sample chunk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.4449275362318841}\n",
      "{'score': 0.0}\n",
      "{'score': 0.4842995169082126}\n",
      "{'score': 0.525672877846791}\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"pairwise_string_distance\")\n",
    "\n",
    "print(evaluator.evaluate_string_pairs(prediction=\"Janky\", prediction_b=chunk)) # This should be somewhat close to 0.0\n",
    "\n",
    "print(evaluator.evaluate_string_pairs(prediction=chunk, prediction_b=chunk)) # This should be 0.0 or very close to it\n",
    "\n",
    "print(evaluator.evaluate_string_pairs(prediction=\"pristine\", prediction_b=chunk)) # This should be further from 0.0\n",
    "\n",
    "print(evaluator.evaluate_string_pairs(prediction=\"brother\", prediction_b=chunk)) # This should be even further from 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vector Database\n",
    "\n",
    "Now we want to start creating our vector database. This is our LLM's cheatsheet of information that it will use in the future to respond to user queries.\n",
    "\n",
    "We will do this by using [Chromadb](https://www.trychroma.com/), which is a vector database!\n",
    "\n",
    "Let's first set up some variables and clear out any existing items in it (you only need to do this if you're doing a fresh run with brand new data, otherwise we can keep this code commented out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the database for our initial run in case it exists.\n",
    "# if os.path.exists(chroma_path):\n",
    "#   shutil.rmtree(chroma_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's start up our chroma db! For this you should have already run this command in the terminal from the README!\n",
    "\n",
    "`chroma run --host localhost --port 8000 --path ./chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to ChromaDB\n",
      "✅ Collection 'llm_rag_collection' ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize our chromadb client locally with special port number so we don't conflict with other things running\n",
    "try:\n",
    "    client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "    # Test the connection\n",
    "    client.heartbeat()\n",
    "    print(\"✅ Successfully connected to ChromaDB\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to ChromaDB: {e}\")\n",
    "    print(\"Make sure ChromaDB is running with: chroma run --host localhost --port 8000 --path ./chroma\")\n",
    "    raise\n",
    "\n",
    "collection_name = \"llm_rag_collection\"\n",
    "\n",
    "# Create collection WITHOUT embedding function for now (we'll handle embeddings manually)\n",
    "try:\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    print(f\"✅ Collection '{collection_name}' ready\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create/get collection: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to ChromaDB\n",
    "\n",
    "Next Let's actually add our chunks to chroma! We'll start by calculating chunk ids so we can update our data at any time. It takes ~8 seconds for 500 records to be embedded and placed into our database. Since our dataset contains over 600k chunks, this would take ~2 to 3 hours! Instead, we'll only add 500 documents, but you can feel free to adjust this number as you deem fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 documents...\n",
      "📚 Processing documents...\n",
      "Number of existing documents in collection: 900\n",
      "✅ No new documents to add\n"
     ]
    }
   ],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    \"\"\"Calculate deterministic chunk IDs using SHA256 hash\"\"\"\n",
    "    chunks_with_id = []\n",
    "    for chunk in chunks:\n",
    "        # Use SHA256 for deterministic, secure hashing\n",
    "        chunk_content = chunk.page_content.encode('utf-8')\n",
    "        chunk_id = hashlib.sha256(chunk_content).hexdigest()\n",
    "        \n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "        chunks_with_id.append(chunk)\n",
    "\n",
    "    return chunks_with_id\n",
    "\n",
    "\n",
    "def add_to_chroma(chunks: list[Document], embedding_func):\n",
    "    \"\"\"Add documents to ChromaDB with error handling and batch processing\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"⚠️ No chunks provided to add\")\n",
    "        return\n",
    "        \n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "    \n",
    "    try:\n",
    "        # Retrieve existing IDs from the collection\n",
    "        existing_items = collection.get(include=[])\n",
    "        existing_ids = set(existing_items[\"ids\"])\n",
    "        print(f\"Number of existing documents in collection: {len(existing_ids)}\")\n",
    "\n",
    "        # Prepare data for new documents\n",
    "        new_chunk_ids = []\n",
    "        new_documents = []\n",
    "        new_metadatas = []\n",
    "        new_embeddings = []\n",
    "\n",
    "        # Process chunks in batches for embedding\n",
    "        batch_size = 10\n",
    "        for i in range(0, len(chunks_with_ids), batch_size):\n",
    "            batch_chunks = chunks_with_ids[i:i+batch_size]\n",
    "            \n",
    "            # Get texts to embed\n",
    "            texts_to_embed = []\n",
    "            chunks_to_add = []\n",
    "            \n",
    "            for chunk in batch_chunks:\n",
    "                chunk_id = chunk.metadata[\"id\"]\n",
    "                if chunk_id not in existing_ids:\n",
    "                    texts_to_embed.append(chunk.page_content)\n",
    "                    chunks_to_add.append(chunk)\n",
    "            \n",
    "            if texts_to_embed:\n",
    "                # Get embeddings for this batch\n",
    "                try:\n",
    "                    batch_embeddings = embedding_func(texts_to_embed)\n",
    "                    \n",
    "                    # Add to our lists\n",
    "                    for j, chunk in enumerate(chunks_to_add):\n",
    "                        new_chunk_ids.append(chunk.metadata[\"id\"])\n",
    "                        new_documents.append(chunk.page_content)\n",
    "                        new_metadatas.append(chunk.metadata)\n",
    "                        new_embeddings.append(batch_embeddings[j])\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error getting embeddings for batch {i//batch_size + 1}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if new_chunk_ids:\n",
    "            print(f\"👉 Adding new documents: {len(new_chunk_ids)}\")\n",
    "            # Add documents in batches to avoid memory issues\n",
    "            add_batch_size = 100\n",
    "            for i in range(0, len(new_chunk_ids), add_batch_size):\n",
    "                batch_ids = new_chunk_ids[i:i+add_batch_size]\n",
    "                batch_docs = new_documents[i:i+add_batch_size]\n",
    "                batch_metas = new_metadatas[i:i+add_batch_size]\n",
    "                batch_embeds = new_embeddings[i:i+add_batch_size]\n",
    "                \n",
    "                collection.add(\n",
    "                    ids=batch_ids,\n",
    "                    documents=batch_docs,\n",
    "                    metadatas=batch_metas,\n",
    "                    embeddings=batch_embeds\n",
    "                )\n",
    "                print(f\"✅ Added batch {i//add_batch_size + 1}: {len(batch_ids)} documents\")\n",
    "        else:\n",
    "            print(\"✅ No new documents to add\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error adding documents to ChromaDB: {e}\")\n",
    "        raise\n",
    "\n",
    "# Make this configurable - can be adjusted based on requirements\n",
    "how_many_documents_to_add = int(os.getenv('DOCUMENTS_TO_ADD', '500'))\n",
    "print(f\"Processing {how_many_documents_to_add} documents...\")\n",
    "\n",
    "# Process documents\n",
    "print(\"📚 Processing documents...\")\n",
    "try:\n",
    "    # Make sure collection and embedding function are defined\n",
    "    if 'collection' not in globals():\n",
    "        print(\"❌ Collection not defined! Please run the ChromaDB connection cell first.\")\n",
    "    elif 'embedding_function' not in globals():\n",
    "        print(\"❌ Embedding function not defined! Please run the embedding function cell first.\")\n",
    "    else:\n",
    "        add_to_chroma(chunks[:how_many_documents_to_add], embedding_function)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to add documents: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data in Vector Database\n",
    "\n",
    "Let's check that our data was successfully added to the vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 900\n",
      "\n",
      "Testing query: 'What does 'jank' mean?'\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. word: Janky\n",
      "definition: Undesirable; less-than optimum....\n",
      "\n",
      "2. word: Janky\n",
      "definition: Far from perfect; messed up...\n",
      "\n",
      "3. word: Janky\\ndefinition: Far from perfect; messed up...\n"
     ]
    }
   ],
   "source": [
    "# Verify the collection has data\n",
    "collection_count = collection.count()\n",
    "print(f\"Total documents in collection: {collection_count}\")\n",
    "\n",
    "# Test a query\n",
    "if collection_count > 0:\n",
    "    test_query = \"What does 'jank' mean?\"\n",
    "    print(f\"\\nTesting query: '{test_query}'\")\n",
    "    \n",
    "    # Get embedding for query\n",
    "    query_embedding = embedding_function([test_query])[0]\n",
    "    \n",
    "    # Search for similar documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTop 3 results:\")\n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        print(f\"\\n{i+1}. {doc[:200]}...\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No documents in collection yet. Please run the data loading cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You've successfully set up your RAG system! The vector database is now populated with slang definitions.\n",
    "\n",
    "### Next Steps:\n",
    "1. You can now close this notebook\n",
    "2. Open the React frontend at http://localhost:3000\n",
    "3. Start asking questions about slang terms!\n",
    "\n",
    "### Tips:\n",
    "- The more documents you add (by increasing `DOCUMENTS_TO_ADD`), the better the RAG system will perform\n",
    "- You can re-run the data loading cell to add more documents\n",
    "- The embeddings are cached, so duplicate documents won't be added twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
