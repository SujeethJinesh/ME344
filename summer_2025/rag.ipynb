{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM + RAG\n",
    "\n",
    "Now we're ready to create an LLM + RAG Pipeline! A large portion of this code was adapted from pixegami, specifically the following two videos:\n",
    "\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "https://www.youtube.com/watch?v=2TJxpyO3ei4\n",
    "\n",
    "## Importing Packages\n",
    "We will first start by importing all of the relevant packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import chromadb\n",
    "\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "from chromadb.db.base import UniqueConstraintError\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.evaluation import load_evaluator\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "chroma_path=\"chroma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Cleaned Data\n",
    "\n",
    "We'll load our cleaned data in the data folder. In our case, we'll be loading in slang data from urban dictionary as a CSV. We encourage you to check out the data to get a sense of how it's laid out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='./data/cleaned_slang_data.csv')\n",
    "slang_document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded in the data, we can take a quick peek at it to see what we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='word: Janky\n",
      "definition: Undesirable; less-than optimum.' metadata={'source': './data/cleaned_slang_data.csv', 'row': 0}\n",
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2580653"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(slang_document[0])\n",
    "print(type(slang_document))\n",
    "print(type(slang_document[0]))\n",
    "len(slang_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have over 2.5 Million Slang items! So we're working with quite a large amount of data!\n",
    "\n",
    "## Chunking our data\n",
    "\n",
    "Now let's go ahead and chunk our data. Remember that this is cutting up the data into manageable chunks so we can fit it into our vector database (cheatsheet for the LLM)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only necessary if we have too much data to add to the context.\n",
    "def split_documents(documents: list[Document]):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=80,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "  )\n",
    "  return text_splitter.split_documents(documents)\n",
    "\n",
    "chunks = split_documents(slang_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the chunks look like. They should be pretty similar but it does definitely help for some words that have very long definitions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='word: Janky\n",
      "definition: Undesirable; less-than optimum.' metadata={'source': './data/cleaned_slang_data.csv', 'row': 0}\n",
      "2706525\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Embedding Function\n",
    "\n",
    "Let's start by creating our embedding function. In our case, since we're using llama3.1, we can use an existing embedding function suited for this from Ollama itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "#   embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "  model_name = \"BAAI/bge-small-en\"\n",
    "  model_kwargs = {\"device\": \"cpu\"}\n",
    "  encode_kwargs = {\"normalize_embeddings\": True}\n",
    "  embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "  )\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what an embedding looks like for reference with our sample chunk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for chunk \"word: Janky\n",
      "definition: Undesirable; less-than optimum.\" is: [0.02469354122877121, -0.04069596901535988, -0.010370255447924137, 0.03439134731888771, 0.010892847552895546, -0.01080682035535574, 0.04962347447872162, 0.023051155731081963, -0.008888516575098038, -0.012985138222575188, 0.0037654428742825985, -0.0407717302441597, 0.03734917566180229, 0.04705151543021202, -0.002829703502357006, 0.0016476493328809738, 0.009689309634268284, 0.03862926736474037, -0.03908330202102661, 0.023559609428048134, 0.03988025337457657, -0.049310773611068726, 0.0049852472729980946, -0.07738950848579407, 0.05985085666179657, 4.8618494474794716e-05, 0.0031844598706811666, -0.003866560524329543, -0.01783439889550209, -0.16132646799087524, -0.016389980912208557, -0.03130568191409111, 0.0541437529027462, 0.0018702696543186903, -0.01178800594061613, 0.005979408044368029, 0.017880897969007492, -0.01939062587916851, -0.0687340795993805, 0.008134184405207634, -0.008391705341637135, 0.05168293043971062, -0.009484957903623581, -0.03485431894659996, -0.006572999060153961, 0.013991276733577251, -0.051540739834308624, -0.009556067176163197, -0.03538709133863449, -0.03566095232963562, 0.02337411232292652, -0.010007980279624462, 0.011682730168104172, 0.02011062204837799, 0.029327018186450005, -0.02091939188539982, 0.05761224031448364, -0.0011575135868042707, -0.008881324902176857, 0.040238652378320694, -0.01939098723232746, 0.006213189102709293, -0.13853317499160767, -0.009142167866230011, 0.04416803643107414, -0.0002677917364053428, 0.02062537707388401, -0.03139154985547066, -0.04191861301660538, 0.06159916892647743, 0.00611698254942894, 0.03777451440691948, -0.02863769605755806, 0.06481243669986725, 0.0016363280592486262, -0.001830321503803134, 0.025532720610499382, -0.011891883797943592, 0.028882233425974846, 0.04761779308319092, -0.007272932678461075, -0.03172167390584946, -0.04769767448306084, -0.04051774740219116, -0.013427778147161007, -0.012093367986381054, -0.04944850131869316, -0.022969728335738182, 0.032918840646743774, 0.016773538663983345, -0.038492366671562195, -0.029189564287662506, 0.03240713104605675, -0.014760646969079971, -0.0400393046438694, -0.00760493241250515, 0.031953513622283936, -0.040673140436410904, -0.044662974774837494, 0.5450738668441772, -0.05927281826734543, 0.040632087737321854, 0.010678540915250778, -0.0669853687286377, 0.010328075848519802, -0.024627085775136948, -0.07114759087562561, -0.024583475664258003, 0.018482891842722893, 0.0051743751391768456, -0.019912634044885635, 0.013672079890966415, 0.07205496728420258, -0.019594283774495125, 0.02825535088777542, -0.06501453369855881, 0.0390976145863533, 0.004220394883304834, 0.019708683714270592, 0.04580673575401306, -0.03130295127630234, 0.044763755053281784, 0.05812913924455643, 0.017080416902899742, -0.0036622462794184685, -0.03900220990180969, 0.018792886286973953, 0.0806695893406868, -0.008664384484291077, -0.014998565427958965, 0.03273586556315422, -0.026429783552885056, 0.0350738950073719, 0.023131323978304863, 0.042367421090602875, 0.012649774551391602, -0.02055951952934265, -0.006697681732475758, -0.026921266689896584, -0.03024151735007763, -0.032793838530778885, -0.07065943628549576, -0.017239803448319435, -0.05759430676698685, -0.049102481454610825, 0.06460562348365784, -0.0508020780980587, -0.013935321941971779, 0.003793420037254691, 0.06374885886907578, 0.02229831926524639, -0.013428092934191227, -0.044518157839775085, -0.046040475368499756, 0.030919507145881653, 0.01924722269177437, 0.026289984583854675, -0.03526989370584488, -0.05542195588350296, -0.04033196344971657, 0.02804398164153099, 0.007471872493624687, -0.055836766958236694, -0.0024623158387839794, 0.005660838447511196, -0.03475533425807953, -0.0029460410587489605, 0.03514824062585831, 0.00014607468619942665, -0.023473428562283516, 0.04333856329321861, -7.464480586349964e-05, -0.07730110734701157, 0.017734361812472343, 0.007135824300348759, -0.0022639851085841656, 0.021051008254289627, -0.017409328371286392, -0.0350002720952034, -0.0033080584835261106, 0.04480517655611038, -0.031205199658870697, -0.04184828698635101, -0.001626187819056213, 0.057302284985780716, 0.029683496803045273, -0.03842351213097572, 0.030824145302176476, 0.0864419937133789, 0.02748318389058113, -0.0470193587243557, 0.010310098528862, -0.05747068673372269, -0.07340768724679947, -0.04377378523349762, 0.014068309217691422, -0.029010307043790817, -0.0008136712713167071, -0.0030206891242414713, -0.04703223705291748, 0.018055981025099754, 0.061696648597717285, 0.020888682454824448, 0.003047055099159479, 0.040672920644283295, 0.04480726644396782, 0.060167353600263596, -0.021874090656638145, 0.023242859169840813, 0.02466917410492897, -0.03720930591225624, -0.036875251680612564, 0.03357318043708801, 0.00875795166939497, -0.03070361725986004, 0.005542324390262365, -0.02902897074818611, -0.0010106207337230444, 0.03722727298736572, -0.0050048972479999065, 0.03774729743599892, -0.020056555047631264, -0.0942295491695404, -0.20570792257785797, 0.03530927747488022, -0.001098170061595738, -0.034301094710826874, 0.06136871874332428, 0.03089456632733345, 0.008522026240825653, -0.03560047596693039, 0.02311638556420803, 0.008407389745116234, 0.013584578409790993, -0.005286655388772488, -0.006203964352607727, 0.02780577726662159, -0.013665436767041683, 0.04660509526729584, 0.017777882516384125, 0.0006777828675694764, -0.023398449644446373, -0.035882387310266495, 0.011091073974967003, 0.03982924297451973, -0.02332174777984619, -0.05963743478059769, 0.018931487575173378, 0.01635979861021042, 0.13611546158790588, 0.05309761315584183, 0.06472133100032806, -0.01613907516002655, 0.00339804426766932, -0.004075522068887949, -0.02600701153278351, -0.0520046167075634, 0.005717593245208263, 0.01737278699874878, 0.020349791273474693, -0.03865250200033188, -0.06147676333785057, -0.020933840423822403, -0.03971205651760101, -0.024310877546668053, -0.02645743265748024, -0.03021690808236599, -0.010219406336545944, -0.01919606700539589, -0.039861418306827545, -0.013219180516898632, -0.05056311935186386, 0.025771912187337875, 0.004216996021568775, -0.008341317996382713, -0.000431165739428252, -0.015881504863500595, 0.030426794663071632, -0.0011797735933214426, -0.07090200483798981, -0.02351078949868679, -0.01015715766698122, 0.02633572369813919, 0.047989603132009506, -0.04472583159804344, 0.06082773208618164, -0.03345262259244919, 0.008344159461557865, -0.006252854131162167, 0.0421210452914238, -0.017803097143769264, 0.016915621235966682, -0.02368430607020855, -0.08191680908203125, 0.14501918852329254, 0.013875632546842098, -0.041575293987989426, 0.05987229943275452, 0.005901705473661423, 0.029292788356542587, -0.010691037401556969, -0.03668394312262535, -0.012784898281097412, 0.023930417373776436, -0.01466057263314724, 0.04173628240823746, 0.006089816801249981, -0.010131043381989002, 0.021858027204871178, 0.05534232780337334, -0.015239440836012363, 0.038102179765701294, 0.022008797153830528, -0.04153841733932495, 0.0294853076338768, 0.035159431397914886, 0.004316070582717657, 0.034681063145399094, -0.040642138570547104, -0.2942655682563782, 0.012500260025262833, -0.008284156210720539, -0.008005132898688316, -0.0037322582211345434, 0.022311981767416, -0.0185799952596426, 0.007211768068373203, -0.07037869840860367, 0.020194023847579956, 0.017188547179102898, 0.03786134347319603, 0.02924993261694908, 0.022610366344451904, 0.008014854043722153, -0.024748465046286583, 0.057946737855672836, -0.00688457814976573, -0.016377033665776253, -0.037523962557315826, 0.047962937504053116, 0.011212917976081371, 0.19639898836612701, -0.032884757965803146, -0.01147452648729086, 0.0008834355976432562, -0.013367953710258007, 0.06572315096855164, -0.05453469231724739, 0.001638670451939106, 0.01117438543587923, 0.04682265967130661, 0.03664960339665413, -0.009469395503401756, 0.021191895008087158, 0.030952798202633858, -0.026893097907304764, 0.010112841613590717, 0.048434849828481674, -0.06349640339612961, -0.08240388333797455, 0.03861941397190094, 0.034777555614709854, 0.02020934596657753, 0.07583089172840118, -0.04415815323591232, 0.013419806025922298, -0.03712178021669388, -0.040950141847133636, 0.0379243828356266, -0.038889382034540176, 0.02241768129169941, 0.04142633080482483, 0.0207638218998909, 0.09037770330905914, -0.007195170037448406, 0.0006052750395610929, -0.023162318393588066, 0.013459691777825356, -0.031546398997306824, 0.006880800239741802, -0.07105036824941635, 0.024249473586678505, 0.028690926730632782, 0.026818716898560524]\n"
     ]
    }
   ],
   "source": [
    "embedding_function = get_embedding_function()\n",
    "\n",
    "chunk = chunks[0].page_content\n",
    "\n",
    "vector = embedding_function.embed_query(chunk)\n",
    "\n",
    "print(f'Vector for chunk \"{chunk}\" is: {vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what does this embedding actually mean? How do we interpret this vector?\n",
    "\n",
    "Remember that this number put in an arbitrary space. It's most useful to think of it as a concept, and if we compare it to other concepts, then the vector difference between the two concepts shows us how similar the two objects are.\n",
    "\n",
    "note: the closer to 0 the evaluation is, the closer the two concepts are!\n",
    "\n",
    "To solidify this concept, let's start by comparing our chunk 0 to various other concepts. Feel free to experiment around as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.3030303030303031}\n",
      "{'score': 0.0}\n",
      "{'score': 0.42781385281385287}\n",
      "{'score': 0.5316017316017316}\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"pairwise_string_distance\")\n",
    "\n",
    "print(evaluator.evaluate_string_pairs(prediction=\"Janky\", prediction_b=chunk)) # This should be somewhat close to 0.0\n",
    "\n",
    "print(evaluator.evaluate_string_pairs(prediction=chunk, prediction_b=chunk)) # This should be 0.0 or very close to it\n",
    "\n",
    "print(evaluator.evaluate_string_pairs(prediction=\"pristine\", prediction_b=chunk)) # This should be further from 0.0\n",
    "\n",
    "print(evaluator.evaluate_string_pairs(prediction=\"brother\", prediction_b=chunk)) # This should be even further from 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vector Database\n",
    "\n",
    "Now we want to start creating our vector database. This is our LLM's cheatsheet of information that it will use in the future to respond to user queries.\n",
    "\n",
    "We will do this by using [Chromadb](https://www.trychroma.com/), which is a vector database!\n",
    "\n",
    "Let's first set up some variables and clear out any existing items in it (you only need to do this if you're doing a fresh run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the database for our initial run in case it exists.\n",
    "if os.path.exists(chroma_path):\n",
    "  shutil.rmtree(chroma_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's start up our chroma db! For this you need to run this command in the terminal!\n",
    "\n",
    "`chroma run --host localhost --port 8000 --path ./chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.get_or_create_collection() got an unexpected keyword argument 'database'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mHttpClient(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m'\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m)\n\u001b[1;32m      3\u001b[0m collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_rag_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchroma_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Client.get_or_create_collection() got an unexpected keyword argument 'database'"
     ]
    }
   ],
   "source": [
    "# Initialize our chromadb client locally with special port number so we don't conflict with other things running\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection_name = \"llm_rag_collection\"\n",
    "\n",
    "collection = client.get_or_create_collection(name=collection_name, embedding_function=get_embedding_function())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Let's actually add our chunks to chroma! We'll start by calculating chunk ids so we can update our data at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ No new documents to add\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[43madd_to_chroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[126], line 31\u001b[0m, in \u001b[0;36madd_to_chroma\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m     28\u001b[0m chunks_with_ids \u001b[38;5;241m=\u001b[39m calculate_chunk_ids(chunks)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Retrieve existing IDs from the collection\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m existing_items \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241m.\u001b[39mget(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     32\u001b[0m existing_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(existing_items[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of existing documents in collection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(existing_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collection' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    # This will calculate \n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        row = chunk.metadata.get(\"row\")\n",
    "        current_page_id = f\"{source}:{row}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Calculate Page IDs\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Retrieve existing IDs from the collection\n",
    "    existing_items = collection.get(include=[\"ids\"])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in collection: {len(existing_ids)}\")\n",
    "\n",
    "    # Prepare data for new documents\n",
    "    new_chunk_ids = []\n",
    "    new_documents = []\n",
    "    new_metadatas = []\n",
    "\n",
    "    for chunk in chunks_with_ids:\n",
    "        chunk_id = chunk.metadata[\"id\"]\n",
    "        if chunk_id not in existing_ids:\n",
    "            new_chunk_ids.append(chunk_id)\n",
    "            new_documents.append(chunk.page_content)\n",
    "            new_metadatas.append(chunk.metadata)\n",
    "\n",
    "    if new_chunk_ids:\n",
    "        print(f\"👉 Adding new documents: {len(new_chunk_ids)}\")\n",
    "        # Add new documents to the collection\n",
    "        collection.add(\n",
    "            ids=new_chunk_ids,\n",
    "            documents=new_documents,\n",
    "            metadatas=new_metadatas\n",
    "            # embeddings will be computed using the embedding function provided when creating the collection\n",
    "        )\n",
    "    else:\n",
    "        print(\"✅ No new documents to add\")\n",
    "\n",
    "add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
